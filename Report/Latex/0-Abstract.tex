\begin{fquote}[RÃ©mi Petitpierre (IAGS)][Paul Guhennec (DHLAB)]Over the 18th and 19th century, the city of Paris is represented by more than 1500 maps and plans. Thanks to state-of-the-art technologies in computer vision, these maps can be vectorized in order to extract the road network. This process relies on CNN-based semantic segmentation and computer vision algorithms for extracting the geometries. In the first part of the project, the student will experiment these techniques. In a second time, the project aims to exploit these data of a great historical value to monitor the evolution of the road network over two centuries, in a period of drastic transformation of the urban fabric. The student will develop quantitative techniques based on graph analysis and network theory to investigate the morphological mutations in the urban fabric. The aim of the project is to better understand the impact of the Haussmanian transformations, and precursory works, on the effectiveness of the transport flow and infrastructure, using a dataset of unprecedented temporal granularity.
\end{fquote}

This project analyzes the urbanization process with the help of the historical maps of Paris.

Maps have errors, especially the one published one hundred years ago. One strategy to understand what Paris looks like historically is to align the average maps we have each year. Therefore a sophisticated alignment algorithm is required.

In this project, two alignment algorithms are discussed. The first algorithm with affine transformation improves the alignment result for all the maps. The second algorithm with homography transformation has better performance and sometimes aligns maps perfectly with each other. However, this algorithm only works 50\% of the time.

% Include the result of the last part.